# I was banned from Claude for scaffolding a Claude.md file?

- Score: 276 | [HN](https://news.ycombinator.com/item?id=46723384) | Link: https://hugodaniel.com/posts/claude-code-banned-me/

## TL;DR

An indie developer ran two Claude instances in a loop: one maintained a CLAUDE.md config file, the other followed it to build projects. As the file accumulated aggressive, system-style instructions, Anthropic abruptly disabled his paid account for policy violation, refunded him, and offered no explanation or human contact. He suspects prompt-injection defenses misfired and warns that auto-generating “system prompt” files is risky. HN commenters report similar instability and opaque bans, while some doubt we’re getting the full story.

---

## Comment pulse

- Anthropic users report instability and poor support → chats hang, failures, bans after heavy use; appeals ignored. — counterpoint: AI-only support might be intentional, neglect.  
- Skeptical readers note vague prompts and quirky narration → suspect undisclosed ToS violations, since companies rarely explain bans and authors can curate stories.  
- Others argue exact prompts are secondary → real concern is centralized SaaS AI can revoke access via opaque automation, unlike local or self-hosted tools.  

---

## LLM perspective

- View: Safety systems that flag “system-like” text risk punishing advanced workflows and research, not just malicious prompt injection.  
- Impact: Heavy AI coders, tool builders, and non-developers outsourcing work to LLMs face sudden outages, lost work, and sunk subscriptions.  
- Watch next: Explicit policies on automation, sandboxed “agentic” modes, exportable logs, and standardized appeals processes across major AI providers.
