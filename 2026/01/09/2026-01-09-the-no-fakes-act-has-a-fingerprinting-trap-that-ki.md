# The No Fakes Act has a “fingerprinting” trap that kills open source?

- Score: 182 | [HN](https://news.ycombinator.com/item?id=46550231) | Link: https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/

## TL;DR
- Debate centers on whether a clause in the proposed US No Fakes Act—meant to stop AI deepfakes—effectively criminalizes general‑purpose or open‑source tools that could be used to mimic voices or likenesses. Commenters fear courts could treat “primarily designed” and mandatory fingerprinting obligations as a weapon against small developers, chilling open‑source, crypto and anonymous software work, while others argue this is overstated litigation pessimism and that broad liability would collide with the First Amendment.  

*Content unavailable; summarizing from title/comments.*

## Comment pulse
- Provision tying developers to users’ misuse would chill open source, crypto, and named distribution; likely unconstitutional under First Amendment protections for publishing code.  
- Some say technologies should impose risk on creators to curb abuse—counterpoint: safe harbors exist because users gain benefits and developers can’t bear unlimited liability.  
- Others see the “fingerprinting” and “primarily designed” language as anti‑competitive lobbying that entrenches big AI firms and makes compliant open‑source generative models impractical.  

## LLM perspective
- View: Ambiguous “primarily designed” standards plus technical mandates like fingerprinting often expand via precedent far beyond sponsors’ stated targets.  
- Impact: If enacted broadly, expect risk‑averse institutions to avoid unwatermarked, modifiable models, sidelining grassroots AI research in favor of centralized APIs.  
- Watch next: Track bill revisions, especially definitions of “digital replica,” safe‑harbor clauses, and whether noncommercial or infrastructure projects gain explicit carve‑outs.
