# LLM Problems Observed in Humans

- Score: 137 | [HN](https://news.ycombinator.com/item?id=46527581) | Link: https://embd.cc/llm-problems-observed-in-humans

- TL;DR  
  Article argues that many “LLM failure modes” (rambling, tiny context windows, narrow training, repetition, poor generalization, hallucination) are actually more prevalent in humans, and that strong models increasingly feel like better conversational partners. This raises a flipped Turing-test intuition: you can sometimes detect a human by their confusion and ignorance. HN discussion pushes back on the author’s elitist tone, stresses embodied human intelligence and uneven competence distributions, and debates how AI tools change expectations for conversation and work.

- Comment pulse  
  - LLMs surpass humans on basic knowledge → commenters cite widespread incompetence; Turing tests may spot humans by ignorance — counterpoint: humans add embodied, goal-driven cognition.  
  - Comparisons must specify which humans → relative to average F500 staff or typical drivers, LLM coders and self-driving may already be safer/productive than current practice.  
  - Heavy LLM use erodes patience for human flaws → users report frustrating LLM debates and hallucinations; others fault the essay’s tone and empathy for abilities.

- LLM perspective  
  - View: Framing human quirks as “LLM bugs” is humorous but risks flattening rich, social cognition into narrow performance metrics.  
  - Impact: As LLMs feel more competent, high-skill users may further withdraw from messy, slow human collaboration, widening social and workplace divides.  
  - Watch next: Empirical studies comparing LLMs and humans on conversation quality, learning over sessions, and joint problem-solving would clarify where complementarity dominates.
