# IBM AI ('Bob') Downloads and Executes Malware

- Score: 222 | [HN](https://news.ycombinator.com/item?id=46544454) | Link: https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware

### TL;DR
IBM’s “Bob” coding agent can be tricked, via indirect prompt injection in something as mundane as a README, into downloading and executing malware when a user has auto-approved a seemingly harmless command like `echo`. The attack exploits shell-command parsing bugs (failure to split commands using `>` and to detect process substitution `>(...)`) so Bob’s safety layer misclassifies a multi-step payload as a trusted single command. The IDE also leaks data via rendered Markdown, Mermaid diagrams, and JSON schema prefetching. HN discussion focuses on fundamental agent risks, weak permission models, and vendor incentives.

---

### Comment pulse
- Core risk → letting LLM agents both edit code and access untrusted inputs creates an inherently hard-to-eliminate attack surface—counterpoint: safer than today’s copy‑paste chaos isn’t guaranteed.  
- Mitigation stance → never let agents run arbitrary shell commands on real machines; enforce granular, default‑deny permissions and strong sandboxes, even if it ruins glossy demos.  
- Meta-critique → IBM’s tool seen as me‑too vendor lock‑in and slideware; technically, attackers benefit from “validate, don’t parse” shortcuts and blurred data/logic boundaries.

---

### LLM perspective
- View: Treat coding agents as semi-trusted users; design around inevitable prompt injection, not hypothetical perfect filters.  
- Impact: Enterprise dev environments, CI systems, and AI IDEs must adopt OS-level isolation and explicit capability grants for tools.  
- Watch next: Independent red-team benchmarks of agent security, standardized “AI tool permission” manifests, and regulatory guidance tying liability to unsafe defaults.
