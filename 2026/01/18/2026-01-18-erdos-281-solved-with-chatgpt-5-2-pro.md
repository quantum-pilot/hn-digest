# Erdos 281 solved with ChatGPT 5.2 Pro

- Score: 288 | [HN](https://news.ycombinator.com/item?id=46664631) | Link: https://twitter.com/neelsomani/status/2012695714187325745

- TL;DR  
  ChatGPT 5.2 Pro produced a detailed proof for Erdős problem 281, which Terence Tao initially treated as a clear, nontrivial AI math contribution. Subsequent checking showed the result already followed from a 1936 Davenport–Erdős paper, so the AI work is now classified as an independent rediscovery rather than a new theorem. The discussion centers on the “long tail” of Erdős problems as AI testbeds, the value of rediscovery, and tension between genuine advances and over-hyped reliability claims.

  *Content unavailable; summarizing from title/comments.*

- Comment pulse  
  - Erdős problems span trivial to famously hard; many low-citation ones are ‘long-tail’ low-hanging fruit, ideal for AI experimentation but not central breakthroughs.  
  - Finding a forgotten 1936 proof shows this ‘solution’ is mainly literature rediscovery, not new math—counterpoint: independent proofs can still clarify structure and test tools.  
  - LLMs impress in avoiding subtle logical errors yet remain unreliable in code and apps; robust tooling, tests, and verification are seen as essential complements.

- LLM perspective  
  - View: This is early evidence that frontier LLMs can handle nontrivial real-analysis-style arguments, not just textbook-level exercises.  
  - Impact: Short-term, expect AI copilots for literature search and proof sketching on under-explored problems, freeing humans for concept formation.  
  - Watch next: Benchmarks on harder Erdős problems and systems that auto-run formal proof checkers, translating informal AI arguments into Lean/Coq.
