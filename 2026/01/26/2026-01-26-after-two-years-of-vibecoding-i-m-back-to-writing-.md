# After two years of vibecoding, I'm back to writing by hand

- Score: 604 | [HN](https://news.ycombinator.com/item?id=46765460) | Link: https://atmoio.substack.com/p/after-two-years-of-vibecoding-im

### TL;DR
An engineer describes two years of relying on AI agents for coding: early delight at rapid features and refactors, then disillusionment after reading the whole codebase and seeing convincing‑looking but messy, inconsistent code. He finds that fixed upfront specs don’t work because real designs must evolve with implementation, which agents can’t yet steward. He’s gone back to mostly hand‑writing code. Commenters debate where AI truly helps, how it erodes or builds skills, and whether better “agent management” changes the outcome.

### Comment pulse
- Education harm is a major worry: AI lets students skip struggle, yielding grads who know theory but can’t explain “their” projects or think in code.  
- Practitioners report two realities: some never get working code; others, with tools like Beads/Cursor rules and Opus 4.5, achieve big gains via iterative supervision.  
- LLMs help with tedious code and refactors but often duplicate logic, miss edge cases, and write weak tests—counterpoint: some blame human specs and review instead.

### LLM perspective
- View: Use agents as powerful juniors for localized changes and exploration, while humans own evolving architecture, invariants, and long-term coherence.  
- Impact: Teams must cultivate debugging and design expertise, not reward people solely for operating AI tools like Cursor or Copilot.  
- Watch next: Studies of long-lived AI-written systems, stronger automatic tests, and IDEs that track whole-project invariants rather than isolated files.
