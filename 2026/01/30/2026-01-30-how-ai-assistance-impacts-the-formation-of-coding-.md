# How AI assistance impacts the formation of coding skills

- Score: 399 | [HN](https://news.ycombinator.com/item?id=46820924) | Link: https://www.anthropic.com/research/AI-assistance-coding-skills

### TL;DR
Anthropic ran a randomized trial with 52 Python devs learning the Trio async library, with or without an AI assistant. AI users finished only ~2 minutes faster on average, but scored 17% lower on immediate quizzes, especially debugging, implying weaker conceptual understanding and code reading. Interaction style mattered: heavy code delegation hurt learning, while using AI for explanations and conceptual questions preserved or improved mastery. HN discussion centers on conflicts of interest, long‑term workforce competence, and whether AI shifts juniors toward specification rather than coding skills.

---

### Comment pulse
- Corporate study, handle skeptically → Anthropic sells tools and publishes harms; commenters want independent replication, stressing outcomes hinge on usage style, not inevitable skill loss.  
- Over‑reliance risks brittle teams → worry juniors become prompt‑gatekeepers unable to debug or deploy when AI, CI/CD, or connectivity fails—counterpoint: work already assumes always‑online services.  
- Skill profile may just shift → some expect seniors’ debugging edge to grow, juniors to focus on specs/system design, while leadership chases velocity over craftsmanship.  

---

### LLM perspective
- View: Treat AI like a calculator in math—explicitly separate “practice without help” time from “assisted production” time for juniors.  
- Impact: Organizations should track not just output speed but debugging performance on unseen bugs to detect over‑reliance on generated code.  
- Watch next: Longer‑term, larger‑N replications across domains, plus experiments comparing AI help versus human mentors for on‑the‑job skill formation.
