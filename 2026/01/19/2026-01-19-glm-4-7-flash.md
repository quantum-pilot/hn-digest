# GLM-4.7-Flash

- Score: 316 | [HN](https://news.ycombinator.com/item?id=46679872) | Link: https://huggingface.co/zai-org/GLM-4.7-Flash

### TL;DR
GLM-4.7-Flash is a 31B-parameter MoE chat/coding model (MIT-licensed) positioned as a lightweight, “Haiku-class” version of GLM‑4.7. Benchmarks show very strong reasoning, coding (SWE-bench Verified 59.2), browsing, and math performance versus other ~30B open models, approaching smaller closed-source models. It’s designed for low-latency, high-throughput use and supports vLLM, SGLang, transformers, and multiple GGUF quantizations, making it viable to self-host on a single high-end GPU, though prompt templates and tooling are still maturing.

---

### Comment pulse
- Flash as Claude Haiku replacement → Z.ai coding plan users report strong, reliable coding results at much lower cost than Claude Pro.  
- Local use is practical → 4-bit GGUF quants run on 24–32 GB GPUs via llama.cpp / Ollama / LM Studio; chat templates still rough — counterpoint: some outputs currently “garbage.”  
- Benchmarks vs reality debated → impressive SWE-bench and UI demos excite some; others say open models just distill SOTA and lag ~1 year, with benchmarks poorly matching real workloads.

---

### LLM perspective
- View: Flash narrows the gap between open 30B-class models and proprietary “mini” coders for realistic coding and browsing workflows.  
- Impact: Indie devs and small teams gain a credible, locally-runnable alternative for test generation, bug-fixing, and long-context reasoning.  
- Watch next: Stable GGUF templates, independent SWE-bench-style evals, and comparisons against GPT‑5.x‑mini / Claude Haiku on large, real projects.
