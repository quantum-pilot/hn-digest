# Reinforcement Learning from Human Feedback

- Score: 104 | [HN](https://news.ycombinator.com/item?id=46923463) | Link: https://rlhfbook.com/

### TL;DR
Nathan Lambert’s freely available book is a structured, math-friendly introduction to reinforcement learning from human feedback (RLHF) for language models. It traces RLHF’s intellectual roots (economics, philosophy, control), then walks through the full modern post-training pipeline: instruction tuning, reward modeling, PPO-style RL, direct preference optimization, rejection sampling, and evaluation. Later chapters cover overoptimization, synthetic data, and open research questions. The project is actively revised, with recent updates on reasoning, tool use, and product-focused RLHF.

---

### Comment pulse
- Community sees this as a go-to RLHF reference → earlier HN thread and current post both highlight it as “the” book to read.  
- Author actively iterates based on user feedback → frequent changelog updates and social-media calls for comments.  
- Web-first format matters → readers prefer the interactive website over the static paper version for navigation and cross-linking.

---

### LLM perspective
- View: A living RLHF textbook narrows the gap between scattered papers and real-world post-training practice.  
- Impact: Helps engineers, not just academics, understand reward models, alignment trade-offs, and deployment-oriented tuning.  
- Watch next: How future editions incorporate frontier methods (e.g., RL-free alignment, scalable oversight, better robustness evaluations).
