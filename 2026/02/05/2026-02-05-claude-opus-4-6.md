# Claude Opus 4.6

- Score: 1399 | [HN](https://news.ycombinator.com/item?id=46902223) | Link: https://www.anthropic.com/news/claude-opus-4-6

## TL;DR
Anthropic’s Claude Opus 4.6 is a major upgrade to its flagship model, emphasizing long-horizon “agentic” work and coding. It adds a 1M-token context window (beta), 128k-token outputs, improved code understanding/debugging, and top performance on real-world-ish benchmarks like Terminal-Bench 2.0, GDPval-AA, and BrowseComp. The API gains adaptive thinking, effort controls, and automatic context compaction; Claude Code gets multi-agent “agent teams” plus memory. HN discussion focuses on practical tests, benchmark one‑upmanship vs OpenAI, and the unclear economics of such rapid iteration.

---

## Comment pulse
- Context vs memory confusion → Users try “needle-in-haystack” Harry Potter tests to probe 1M context, but others note the model may just recall pretraining knowledge.  
- Benchmark arms race → Anthropic’s claims are quickly challenged by OpenAI’s new Codex scores; commenters question benchmark quality, saturation, and runtime variability—counterpoint: broader evals still favor Opus 4.6.  
- Business model skepticism → Many think per-token inference is profitable but worry flat-rate “vibecoding” plans are subsidized and may become pricier as training costs and cadence rise.

---

## LLM perspective
- View: 4.6 is about durable, tool-using agents more than raw chat IQ; context, compaction, and effort controls all point there.  
- Impact: Stronger long-context coding and Office integrations shift real work—especially maintenance, analysis, and reporting—onto AI copilots and back-office automations.  
- Watch next: Independent multi-benchmark bake-offs, usage-based vs flat pricing for dev tools, and whether safety probes scale with rapidly improving cyber capabilities.
