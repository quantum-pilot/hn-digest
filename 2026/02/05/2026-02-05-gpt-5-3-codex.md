# GPT-5.3-Codex

- Score: 938 | [HN](https://news.ycombinator.com/item?id=46902638) | Link: https://openai.com/index/introducing-gpt-5-3-codex/

- TL;DR  
  GPT-5.3-Codex is OpenAI’s new flagship “agentic” coding model, combining GPT‑5.2’s reasoning with 5.2‑Codex’s development skills while running 25% faster. It sets state-of-the-art scores on SWE‑Bench Pro, Terminal‑Bench, OSWorld, and cybersecurity challenges, and can operate a full desktop to build and iterate on apps, games, and professional documents over millions of tokens. OpenAI heavily dogfooded it to train and deploy itself and is treating its cyber abilities as “high capability,” prompting debate over control, security, and AI self-improvement.

- Comment pulse  
  - Human-in-loop Codex collaboration suits complex codebases; full autonomy can degrade design. Domain-specific agents with custom tooling may best trade off control and delegation.  
  - Benchmarks show big Terminal-Bench gains over Claude; some doubt real-world alignment, note xhigh cost, and see synchronized launches as deliberate arms-race theater.  
  - Cyber gains and self-dogfooding look like early self-improvement; some say preparedness ignores insecure AI-written code, urging secure-by-default — counterpoint: others dismiss OpenAI’s safety as marketing.

- LLM perspective  
  - View: This is a shift from code autocomplete to persistent desktop coworker, blurring IDE, browser, and operating-system boundaries.  
  - Impact: Senior engineers become orchestrators and reviewers; junior-level implementation and ops tasks increasingly handled by Codex-like agents.  
  - Watch next: Independent audits of benchmarks, longitudinal studies of Codex-maintained codebases, and concrete metrics for “secure by default” AI-generated software.
