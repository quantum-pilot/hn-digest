# Gemini 3 Deep Think

- Score: 578 | [HN](https://news.ycombinator.com/item?id=46991240) | Link: https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/

### TL;DR
Gemini 3 Deep Think is Google’s upgraded “reasoning mode” for Gemini 3, aimed at hard scientific, mathematical, and engineering problems where data is messy and answers aren’t unique. It now reaches gold-medal performance on 2025 math, physics, and chemistry Olympiads, scores highly on ARC-AGI-2 and Humanity’s Last Exam, and hits 3455 Elo on Codeforces. Early users report it catching subtle math-paper errors, designing crystal-growth protocols, and turning sketches into 3D-printable designs. HN debates benchmark meaning, model-release velocity, and Google’s lagging product UX.

---

### Comment pulse
- Benchmark feats impress (ARC-AGI-2 jump, Olympiad gold), but many argue this reflects better spatial/pattern reasoning, not a sudden leap toward general intelligence.

- Rapid-fire releases from Google, OpenAI, and Chinese labs blur what’s a new model vs a “mode” or agent stack, leaving developers confused about capabilities and categories.

- Some see Google “running away” on capabilities; others report Gemini products forgetting context, mixing languages, and giving weak advice — counterpoint: API users still get strong, cost-effective results.

---

### LLM perspective
- View: Deep Think looks like heavy multi-step, best-of-N reasoning on top of a strong base model, tuned for math/science workflows.

- Impact: Most immediate gains go to researchers and engineers who can formalize problems and cheaply explore many candidate solutions.

- Watch next: Independent evals on real lab/engineering tasks, open tools for agentic workflows around Gemini 3, and whether UX catches up to raw capability.
