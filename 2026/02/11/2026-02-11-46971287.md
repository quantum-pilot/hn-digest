# Show HN: I taught GPT-OSS-120B to see using Google Lens and OpenCV

- Score: 39 | [HN](https://news.ycombinator.com/item?id=46971287) | Link: https://news.ycombinator.com/item?id=46971287

### TL;DR
A developer wraps GPT-OSS-120B, a local text-only LLM, with OpenCV and Google Lens to give it “pseudo-vision”: images are sent to Lens, labels and matches come back as text, which the model reasons over. HN commenters argue this mostly chains external APIs, questioning whether the LLM adds value versus directly calling a multimodal cloud model. Others like the one-command setup for small local models but worry about Google ToS violations, fragility, and prefer proper vision-trained or local VLM alternatives.  

*Content unavailable; summarizing from title/comments.*

---

### Comment pulse
- Tool-chaining critique → Google Lens does all vision; LLM reads labels, like Wolfram Alpha for math — counterpoint: local tool orchestration can still be useful.  
- Alternative designs → Commenters suggest real vision training or existing local VLMs; author says goal is one-pip install for 1B–20B models, with vision variants coming.  
- ToS and robustness → Scraping Google/Lens likely violates terms and is brittle; commenters recommend official APIs like Custom Search or SerpAPI, especially beyond personal tinkering.  

---

### LLM perspective
- View: This pattern turns any text-only LLM into a multimodal agent by delegating perception and retrieval to external specialized services.  
- Impact: Most value is in the ecosystem layer—tool plugins, routing, and UX—rather than ever-larger base models for many practical workflows.  
- Watch next: Compare this hack against native VLMs on latency and accuracy; monitor Google’s response to Lens-based usage at scale.
