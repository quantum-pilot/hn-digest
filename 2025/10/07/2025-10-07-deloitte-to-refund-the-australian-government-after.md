# Deloitte to refund the Australian government after using AI in $440k report

- Score: 429 | [HN](https://news.ycombinator.com/item?id=45500485) | Link: https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report

- TL;DR
  - Deloitte will refund the Australian government after admitting it used generative AI (Azure OpenAI GPT‑4o) on a $440k review that contained hallucinated citations and an erroneous court summary. The report, on DEWR’s welfare-compliance automation, flagged punitive design and poor rule-to-law traceability; DEWR says recommendations stand, with only footnotes corrected. HN debates center on harms from faulty government IT, consulting firms’ bait‑and‑switch staffing and blame‑shifting, and whether AI‑assisted reports need mandatory disclosure, verification, and stronger quality controls before informing policy.

- Comment pulse
  - AI‑generated analysis in welfare enforcement risks harm → bad citations mask weak evidence; wrongful penalties escalate to debt actions — counterpoint: conclusions align with evidence.
  - Consultancies bait‑and‑switch teams → partners sell; juniors (now aided by LLMs) deliver mediocre work at premium rates, with A/B/Z rotations when scrutiny ebbs.
  - Decision laundering via consultants → executives buy external validation and liability shields; in‑house experts ignored to create optics of action and cover for bad bets.

- LLM perspective
  - View: AI can assist research, but hallucination-prone outputs need human fact-checking and transparent disclosure in client-facing deliverables.
  - Impact: Public-sector procurement will tighten: explicit AI-use clauses, evidence traceability, and penalties for undisclosed automation in policy-critical reports.
  - Watch next: Audits of other consultant reports, DEWR’s follow-through on recommendations, and professional standards for LLM-assisted research and citation verification.
