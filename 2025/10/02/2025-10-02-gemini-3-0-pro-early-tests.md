# Gemini 3.0 Pro – early tests

- Score: 162 | [HN](https://news.ycombinator.com/item?id=45453448) | Link: https://twitter.com/chetaslua/status/1973694615518880236

- TL;DR
    - Early testers—likely A/B’d in Google AI Studio—report Gemini 3.0 Pro rapidly generating complex web UIs and physics demos. Commenters are intrigued but skeptical, asking for reproducible API access and standardized evaluations before judging. Broader thread critiques Google’s muddled product/SDK story despite strong engineering, and contrasts models: Gemini’s high-level reasoning but fussy accuracy/steerability; Claude’s reliability; GPT‑5’s inconsistency. General sentiment: frontier models feel close; usefulness depends on task and workflow; beware AGI‑style hype.
    - Content unavailable; summarizing from title/comments.

- Comment pulse
    - Impressive code demos are circulating → Likely AI Studio A/B test; credibility hinges on public API parity and benchmarks like Pelican.
    - Google lacks product coherence → Fragmented APIs/subscriptions and poor docs slow adoption — counterpoint: big firms avoid risky launches; experimentation is harder under brand scrutiny.
    - No single best model → Gemini: high ceiling, weak token-level accuracy/steerability; Claude: consistent; GPT‑5: erratic; large context+PDF support can be decisive.

- LLM perspective
    - View: Don’t judge by demos; run targeted, repeatable tests for your tasks: structured-output fidelity, tool-calling accuracy, long-context recall, latency/cost.
    - Impact: If steerability and token-level accuracy improve, Gemini could replace Claude for agents/coding; simplified APIs would boost enterprise adoption.
    - Watch next: Official 3.0 model card, stable API, SWE-bench Verified/GPQA/LiveCodeBench results; pricing, context window, PDF ingestion, rate limits.
