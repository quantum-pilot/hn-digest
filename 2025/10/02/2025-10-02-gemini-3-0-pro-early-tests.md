# Gemini 3.0 Pro – early tests

- Score: 193 | [HN](https://news.ycombinator.com/item?id=45453448) | Link: https://twitter.com/chetaslua/status/1973694615518880236

- TL;DR
  Early “Gemini 3.0 Pro” sightings likely come from Google AI Studio A/B tests; flashy code demos impress but spur skepticism and calls for rigorous, standardized evals. Broader thread: Google’s product gaps and tangled AI offerings hinder adoption, despite scale advantages and Android integration. Users see no clear model leader; effectiveness varies by workflow, with Gemini highly productive for some. Overall mood: wary after AGI-hype cycles; perceived 2025 progress favors efficiency and smaller models over headline breakthroughs.
  - Content unavailable; summarizing from title/comments.

- Comment pulse
  - Early demos = A/B tests; flashy code tasks aren’t real evals → prefer standardized benchmarks (e.g., Pelican); many models one-shot these — counterpoint: fidelity improved.
  - Google lacks product culture → fragmented APIs, subscriptions, and confusing UX slow adoption; strong engineering and data could still dominate.
  - No clear model winner → Gemini: high reasoning ceiling but brittle outputs; Claude: consistent and steerable; GPT‑5: inconsistent; Gemini’s large context/PDF ingestion praised.

- LLM perspective
  - View: Early leak demos overweight codegen; evaluate with end-to-end tasks, tool-calling reliability, and long-context retention under real constraints.
  - Impact: If Google simplifies billing, unifies APIs, and polishes UX, developer migration from GPT/Claude could accelerate.
  - Watch next: Official 3.0 specs, independent benchmarks beyond vibecode, context-length stress tests, structured-output accuracy, and API consolidation milestones.
