# EuroLLM: LLM made in Europe built to support all 24 official EU languages

- Score: 556 | [HN](https://news.ycombinator.com/item?id=45733707) | Link: https://eurollm.io/

- TL;DR
  - EuroLLM is an open-source, EU-built multilingual LLM: a 9B base/instruct model trained on 4T tokens across 35 languages (covering all 24 EU languages) plus a 1.7B edge variant. Trained on EuroHPC’s MareNostrum 5, it claims to outperform similar-sized peers and plans multimodal. HN debates EU “pick winners” policy vs research-focused funding, and argues multilingual quality hinges on data balance and tokenization, so EU-centric training can boost non‑English performance. Others note the 2024 release, language-family nuances, and repurposed HPC compute.

- Comment pulse
  - Multilingual quality needs balanced data and smart tokenization → English-heavy corpora degrade non‑English; EU-curated data can help — counterpoint: many frontier models already perform adequately.
  - EU “pick winners” criticized; others argue EuroHPC targets research, not startups, with matched-funding programs elsewhere and broader upskilling benefits over free compute for businesses.
  - Context checks → model first released 2024; commenters list EU languages, note family distinctions; EuroHPC repurposes physics supercomputers for AI training.

- LLM perspective
  - View: A mid-size open multilingual model tailored to EU languages fills a gap between tiny edge and English‑centric models.
  - Impact: Public-sector and SME use in minority languages; on‑device variant enables offline translation, chat, and domain fine‑tunes across Europe.
  - Watch next: Benchmark vs Aya, Qwen-7B, Llama-3.1-8B in non‑English; license clarity; multimodal release; dataset transparency for low-resource languages.
