# Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop

- Score: 236 | [HN](https://news.ycombinator.com/item?id=45771870) | Link: https://news.ycombinator.com/item?id=45771870

- TL;DR
    - HN users share local LLM/coding setups: M‑series Macs with LM Studio/Ollama, Zed or VSCode; CPU boxes and GPU servers via llama.cpp/llama‑swap. Models: gpt‑oss 20B/120B, Qwen3‑Coder‑30B (Q4/A3B); reported ~50 tok/s on 64GB Macs. Workflows use Aider/continue.dev, FIM, and offline RAG projects (e.g., mapping U.S. election laws). Privacy/availability are wins; storage/RAM requirements are real. For heavy coding and long prompts, frontier APIs (Claude, GPT‑*) are faster and cheaper. Handy snippet shared to run gpt‑oss:20b locally.
    - Content unavailable; summarizing from title/comments.

- Comment pulse
    - Local runs span M2/M3 Macs, CPU-only workstations, and GPU servers; coding on laptops is feasible but large prompts slow non‑NVIDIA hardware.
    - Popular models: gpt‑oss 20B/120B, Qwen3‑Coder‑30B; Q4/A3B quantizations fit 24–64GB Macs; expect ~50 tok/s, better FIM with llama.cpp plugins.
    - Tooling: Ollama, llama.cpp + llama‑swap, Aider, continue.dev, VSCode; offline/RAG wins and OpenRouter zero‑retention options — counterpoint: frontier APIs outperform and cost less for heavy coding.

- LLM perspective
    - View: Local LLMs excel at bounded tasks, FIM, and RAG; struggle with long-context coding unless ample GPU memory.
    - Impact: Macs with 24–64GB unify dev+inference; server‑class GPUs remain necessary for 120B models and high‑throughput agents.
    - Watch next: Measure code‑benchmarks (HumanEval, SWE-bench), NPU acceleration on M4/Ryzen AI, and privacy guarantees from OpenRouter/IDE plugins.
