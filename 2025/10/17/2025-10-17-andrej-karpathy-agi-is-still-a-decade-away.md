# Andrej Karpathy – AGI is still a decade away

- Score: 429 | [HN](https://news.ycombinator.com/item?id=45619329) | Link: https://www.dwarkesh.com/p/andrej-karpathy

- TL;DR
  - Karpathy argues “the decade of agents” lies ahead: problems are solvable but hard. Today’s LLMs lack persistent memory, robust computer-use, multimodality, and dependable self-improvement; RL contributes little beyond motor-like domains. Pretraining acts as “crappy evolution,” compressing knowledge and bootstrapping in‑context learning, so models look smart in-session but don’t retain across sessions. Expect continued giant gradient‑descent systems, steady multi-front progress, and coding help as autocomplete rather than automation. HN debated the “march of nines,” brain-analogy overreach, fuzzy AGI definitions, and a possibly misleading title.

- Comment pulse
  - Progress is a “march of nines” → each extra 9 costs similar effort; benchmarks look exponential, systems net linear — counterpoint: AI will add nines.
  - Caution on brain analogies → CS-led neuroscience takes invite hubris; others note AI–neuro cross-pollination and shared concepts, but analogies often mislead public understanding.
  - AGI discourse is ill-posed → definitions vary and hinder bets; some call the title misleading clickbait, echoing prior Sutton episode disputes.

- LLM perspective
  - View: Agents need persistent memory, trusted computer-use, and tighter toolchains; RL remains secondary to representation learning and planning.
  - Impact: Expect productivity uplift via autocomplete and refactors, not autonomous dev teams; orgs should budget multi-year reliability and safety engineering.
  - Watch next: Robust long-horizon continual-learning benchmarks, scalable sparse attention, and credible “computer-use” agents measured on complex, multi-application tasks.
