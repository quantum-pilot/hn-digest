# 'Attention is all you need' coauthor says he's 'sick' of transformers

- Score: 308 | [HN](https://news.ycombinator.com/item?id=45690840) | Link: https://venturebeat.com/ai/sakana-ais-cto-says-hes-absolutely-sick-of-transformers-the-tech-that-powers

- TL;DR
  Llion Jones, a coauthor of Transformers, argued AI research has over-optimized on transformer scaling due to investor/career pressure, narrowing exploration. He’s redirecting Sakana AI toward riskier, nature-inspired work (e.g., synchronization-based “continuous thought”) and urges open, exploratory research to find the next paradigm. HN discussion credits transformers’ massive impact and efficiency but doubts a near-term architectural leap; some see fundraising theater. Others note hardware parallelism and product incentives entrench transformers, suggesting real breakthroughs may require algorithm–hardware co-design and incentive shifts, not simply bigger models.

- Comment pulse
  - Transformers are strong universal approximators; expect marginal architecture gains. Explore new objectives, optimization, or probabilistic/graph models; some report field-specific harms.
  - Jones’s critique is fundraising theater → provocative messaging to attract capital. — counterpoint: curiosity and doing new work also require funding; both can be true.
  - Hardware fit locks in transformers; real change needs algorithm–hardware co‑design over decades — counterpoint: parallelizable algorithms are inherently superior, not merely hardware accidents.

- LLM perspective
  - View: The bottleneck is incentives: exploitation over exploration; reallocating compute and time matters more than slogans about 'post-transformer'.
  - Impact: If exploration budgets rise, smaller labs and academia regain relevance; hardware vendors shift toward co-designed primitives beyond attention.
  - Watch next: Concrete signals: non-attention models beating LLMs on needle-in-haystack/memory tasks; compute grants for alt-architectures; chip roadmaps supporting new sparsity/synchrony ops.
