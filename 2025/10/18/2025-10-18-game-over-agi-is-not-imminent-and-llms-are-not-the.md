# Game over. AGI is not imminent, and LLMs are not the royal road to getting there

- Score: 119 | [HN](https://news.ycombinator.com/item?id=45627171) | Link: https://garymarcus.substack.com/p/the-last-few-months-have-been-devastating

- TL;DR
    - Gary Marcus argues the LLM path to AGI has stalled: distribution-shift failures (Apple/ASU), an underwhelming GPT‑5, and public walk-backs from Sutton, Karpathy, and Hassabis on capabilities hype. He says LLMs remain useful but far from AGI, reiterating his call for alternative, hybrid approaches. HN reaction is mixed: some see this as recycled self-promotion; others note real limits yet significant GPT‑5 gains; debates span symbolic–neural hybrids, whether AGI is even desirable, and why hyperscalers need massive new capital.

- Comment pulse
    - Marcus adds little and self-promotes → repeats critiques; leans on expert tweets, not evidence — counterpoint: symbolic methods persist in hybrids guiding LLMs.
    - LLMs struggle with distribution shift and agency → Apple/ASU findings, Karpathy skepticism; AGI decade away. Opponents cite GPT‑5 gains, expect GPT‑6 to clear 'AGI' bars.
    - Hyperscalers raising hundreds of billions → inference demand and costly frontier R&D; larger models and new hardware loom. Skeptics: it's enrichment/surveillance and spreadsheet-driven valuations.

- LLM perspective
    - View: Current LLMs plateau on distribution shift; progress needs architecture changes, richer objectives, or hybrid neuro-symbolic systems.
    - Impact: AGI timelines stretch; funding pivots to agents, memory, tool-use, evals; infra spend prioritizes profitable inference over moonshot training.
    - Watch next: OOD benchmarks for reasoning/agency, reproducible math results, GPT‑6 and DeepSeek releases, GB200 deployments, investor scrutiny of AI economics.
