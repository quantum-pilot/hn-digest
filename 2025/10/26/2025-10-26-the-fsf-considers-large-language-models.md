# The FSF considers large language models

- Score: 90 | [HN](https://news.ycombinator.com/item?id=45711786) | Link: https://lwn.net/Articles/1040888/

- TL;DR
  - FSF is weighing how free‑software rules should handle LLMs—training, attribution, and whether AI‑assisted code should be allowed. HN debates show: prompt‑disclosure and labeling requirements are impractical in iterative workflows; disputes over training data legality persist (opaque datasets vs fair‑use arguments); and some call AI output “tainted” and unscalable to validate, while others find it useful for boilerplate with human review. There’s frustration that FSF has no new license yet, plus warnings that blanket bans risk excluding developers using LLMs as assistive tech.
  - Content unavailable; summarizing from title/comments.

- Comment pulse
  - Prompt and AI-code labeling mandates are unworkable → iterative chats and edits blur authorship; version-control already tracks collaboration — counterpoint: provenance aids audits.
  - LLM training equals theft → datasets are opaque; outputs may echo licensed code — counterpoint: several rulings treat training as fair use.
  - Ban “tainted” AI code → non-deterministic process and mimicry hide copying; exhaustive validation doesn’t scale — counterpoint: for boilerplate, careful human review is sufficient.

- LLM perspective
  - View: Target norms for disclosure, attribution, and assistive-use exceptions; avoid blanket bans; clarify training and output obligations separately.
  - Impact: Project policies and compliance tools will evolve faster than licenses; FSF guidance sets tone but won’t settle law.
  - Watch next: Court decisions on training fair use; SPDX-style provenance fields; FSF statements on accessibility accommodations and acceptable disclosure practices.
