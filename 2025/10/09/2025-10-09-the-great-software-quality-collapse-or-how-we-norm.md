# The great software quality collapse or, how we normalized catastrophe

- Score: 275 | [HN](https://news.ycombinator.com/item?id=45528347) | Link: https://techtrenches.substack.com/p/the-great-software-quality-collapse

- TL;DR
  - Argues we’re in a software-quality crisis: egregious memory leaks (Apple Calculator 32GB), routine system regressions, and brittle pipelines culminate in outages like CrowdStrike’s $10B failure. AI is a force multiplier—studies cited: 322% more vulns, 45% exploitable code, juniors fail 4x faster—while abstraction stacks and power limits meet a $364B infra splurge instead of engineering fixes. Proposes rewarding efficiency and rebuilding the junior pipeline. HN split: some see formulaic, anecdotal alarmism; others report real decline driven by moats/updates; a few say LLMs already help audits.

- Comment pulse
  - It’s nostalgia; quality tracks incentives → users pay for speed/features; updates changed calculus vs shrink‑wrapped era. — counterpoint: moats reduce pressure, so bugs linger.
  - The piece reads AI-written and anecdotal → formulaic phrasing, shaky “stack chain,” no hard metrics.
  - LLMs can raise quality in review → effective at security/config analysis; may become standard due diligence.

- LLM perspective
  - View: Focus on incentives, not tools; measure and reward efficiency, reliability, and blast radius, or bloat and outages persist.
  - Impact: If power and capex constraints bite, feature velocity slows; ops SLOs and cost budgets become promotion gates.
  - Watch next: Hard data beats vibes: reproducible quality benchmarks, energy budgets per feature, and postmortem transparency from hyperscalers and AI vendors.
