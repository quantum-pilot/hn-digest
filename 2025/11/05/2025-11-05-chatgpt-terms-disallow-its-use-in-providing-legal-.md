# ChatGPT terms disallow its use in providing legal and medical advice to others

- Score: 178 | [HN](https://news.ycombinator.com/item?id=45825965) | Link: https://www.ctvnews.ca/sci-tech/article/openai-updates-policies-so-chatgpt-wont-provide-medical-or-legal-advice/

- TL;DR
  - OpenAI tightened ChatGPT’s usage terms around medical/legal advice: tailored guidance now requires licensed professional involvement. HN debates whether this bans only using ChatGPT to advise others or any first‑party tailored advice. Several users report new refusals, implying a technical guardrail shift. Anecdotes praise ChatGPT’s diagnostic help, while others warn of hindsight bias, inconsistent outputs, and liability. Motives discussed: legal CYA, avoiding medical‑device regulation, and potential paid clinician versions. Some foresee more clinician‑supervised workflows; others plan offline models to bypass restrictions.
  - Content unavailable; summarizing from title/comments.

- Comment pulse
  - Policy scope unclear: "no tailored advice without licensed involvement" vs "only advising others is banned"; some users now see refusals—suggests a behavioral change.
  - Utility vs risk: Anecdotes of rare‑disease hits and education; critics note hindsight bias and variable outputs — counterpoint: still outperforms average primary care for triage.
  - Motives: litigation avoidance and medical‑device exposure; others suspect paywalled professional tiers; brand attribution makes OpenAI central in malpractice stories.

- LLM perspective
  - View: OpenAI is tightening risk perimeter while leaving room for clinician-supervised use.
  - Impact: Consumers lose direct tailored advice; clinicians, EHR vendors, and offline models gain relevance.
  - Watch next: Model guardrails, pro/enterprise medical offerings, FDA/EMA guidance, benchmarked diagnostic performance under refusals.
