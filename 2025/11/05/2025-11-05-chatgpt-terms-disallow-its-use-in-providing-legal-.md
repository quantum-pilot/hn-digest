# ChatGPT terms disallow its use in providing legal and medical advice to others

- Score: 266 | [HN](https://news.ycombinator.com/item?id=45825965) | Link: https://www.ctvnews.ca/sci-tech/article/openai-updates-policies-so-chatgpt-wont-provide-medical-or-legal-advice/

TL;DR
OpenAI clarified that ChatGPT’s terms prohibit developers from using it to deliver tailored legal or medical advice to others, while personal queries remain allowed; the model’s behavior hasn’t changed. HN readers see this as targeting apps that repackage AI as professional counsel, citing safety, liability, and privacy risks. Some report success using ChatGPT to summarize records or reconcile diagnoses, but caution against uploading sensitive data. Discussion splits over ethics vs. competitiveness and whether similar limits should apply to financial or broader “advice.”

Comment pulse
- Policy targets downstream products → building apps giving tailored legal/medical advice is disallowed; personal queries remain allowed — counterpoint: article initially implied all advice was banned.
- It’s reasonable → prevents scammy reselling of AI as professional counsel; reduces liability and harm.
- Useful but risky → people use ChatGPT to parse records; accuracy varies; don't upload sensitive data; seek clinicians/lawyers for decisions.

LLM perspective
- View: This codifies platform risk boundaries; shifts responsibility from model behavior to developer compliance and product design.
- Impact: Third‑party health/legal startups on OpenAI APIs must pivot to summaries, triage, or referrals, not tailored advice.
- Watch next: Policy wording updates, enforcement actions, and any “validated” healthcare/legal offerings with audits, disclaimers, and indemnity.
