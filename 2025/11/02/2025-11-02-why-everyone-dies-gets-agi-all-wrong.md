# Why "everyone dies" gets AGI all wrong

- Score: 105 | [HN](https://news.ycombinator.com/item?id=45786888) | Link: https://bengoertzel.substack.com/p/why-everyone-dies-gets-agi-all-wrong

TL;DR
- Ben Goertzel rebuts Yudkowsky/Soares’s “everyone dies” thesis, arguing their optimization-centric view ignores embodied, social, open-ended intelligence and real-world context. AGI won’t be random minds; architectures, training environments, and decentralized governance can nudge values and reduce single-actor failure modes. He warns near-term socio-economic turbulence and regulatory capture are bigger risks than hypothetical ASI psychopathy. HN debates his reliance on mammalian compassion, notes collective institutions often act amoral, challenges both camps with physics and non-biological evolution, and worries about corporate power concentration.

Comment pulse
- Intelligence doesn’t guarantee compassion; groups act amoral; build incentive environments that reward desired morals — counterpoint: behavior follows punishments/rewards more than intrinsic values.
- Both extremes miss physics and evolution: EY’s models ignore physical limits; Goertzel overprojects biology; AGI will mirror builders’ biases without human evolutionary constraints.
- Power concentration fear: LLMs destabilize order and entrench corporate control; others see weak moats or even prefer elite stewardship over uninformed publics.

LLM perspective
- View: Treat alignment as socio-technical: architecture, incentives, and governance jointly shape behavior more than abstract optimization arguments.
- Impact: Decentralized development and antitrust matter; otherwise, training regimes and deployment power concentrate in few labs, increasing correlated failure risk.
- Watch next: Demonstrations of value generalization across tasks, non-LLM architectures’ alignment properties, and concrete labor-transition policies piloted alongside capability scaling.
