# Major AI conference flooded with peer reviews written by AI

- Score: 174 | [HN](https://news.ycombinator.com/item?id=46088236) | Link: https://www.nature.com/articles/d41586-025-03506-6

### TL;DR
ICLR, a leading machine‑learning conference, discovered through Pangram Labs’ AI‑detection tool that about 21% of its 75k peer reviews appear fully AI‑generated and over half show AI involvement. Only ~1% of submitted papers look fully AI‑written, with 9% containing mostly AI text. Authors report vague, incorrect, or hallucinated feedback affecting acceptance decisions. The findings trigger two debates: collapsing incentives and quality in peer review, and whether AI‑detectors are accurate enough to justify such sweeping claims.

---

### Comment pulse
- Peer review incentives are broken → low pay, career pressure, and paper glut make cursory or AI‑written reviews rational, not exceptional misconduct.  
- AI detectors are distrusted → past tools had high false positives; critics see Pangram’s study as PR—counterpoint: Pangram cites very low error rates and prior baselines.  
- Detection versus proof → tools may support aggregate statistics, but many argue you can’t rigorously prove any single review is AI‑generated, limiting enforcement.

---

### LLM perspective
- View: Conferences should explicitly allow assistive AI with disclosure, but treat fully automated, unvetted reviews as misconduct.  
- Impact: Stronger policies and spot checks will push serious reviewers toward “AI‑augmented but accountable,” marginalizing pure copy‑paste behavior.  
- Watch next: Independent benchmarking of detectors on scholarly text and conferences publishing AI‑use stats and enforcement outcomes.
