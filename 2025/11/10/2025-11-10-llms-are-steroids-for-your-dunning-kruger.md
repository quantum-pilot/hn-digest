# LLMs are steroids for your Dunning-Kruger

- Score: 294 | [HN](https://news.ycombinator.com/item?id=45876744) | Link: https://bytesauna.com/post/dunning-kruger

- TL;DR
    - The essay argues LLMs act less like “knowledge engines” and more like “confidence engines,” amplifying users’ ideas—sometimes into fluent, convincing wrongness. The author flags habit-forming reliance and broader societal effects when machines speak our native medium, language. HN replies split: some feel LLMs sap confidence; others see overconfident misuse (e.g., bogus configs, hallucinated APIs). Many treat LLMs as scaffolding or a thinking mirror, but warn that smooth explanations can crowd out the productive struggle real learning needs.

- Comment pulse
    - LLMs breed overconfidence → Fluent answers feel right; support teams see chatbot-made configs and hallucinated endpoints. — counterpoint: others less sure, using LLMs as crutch.
    - Great for bootstrapping → Newcomers gain vocabulary and starting points; but plausibility masks errors, and domain experts ask harder questions where models falter.
    - Smoothness ≠ understanding → Passive, polished explanations feel good but reduce learning; active struggle works better (PNAS). Risk: LLMs replace practice; confident misdiagnoses happen.

- LLM perspective
    - View: Treat LLMs as confidence amplifiers; add friction: sources, uncertainty, abstention, and prompts that demand verification or structured outputs.
    - Impact: Overconfidence harms novices and support desks; disciplined users gain leverage; education risks more passive learning unless curricula adapt.
    - Watch next: Measure calibration: user overconfidence rates, abstention accuracy, provenance UX; enterprise policies for risky tasks; benchmarks beyond factual accuracy.
