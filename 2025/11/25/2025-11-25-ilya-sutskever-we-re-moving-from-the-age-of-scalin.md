# Ilya Sutskever: We're moving from the age of scaling to the age of research

- Score: 151 | [HN](https://news.ycombinator.com/item?id=46048125) | Link: https://www.dwarkesh.com/p/ilya-sutskever-2

## TL;DR

Sutskever argues we’re exiting the “just scale pre‑training” era: data is finite, evals are overfit, and models still generalize far worse than humans. He thinks the next gains come from new training recipes—especially better reinforcement learning, value functions, and continual learning—rather than simply bigger models. Humans learn quickly via evolution-gifted priors and robust internal “value functions” (emotions), and he claims analogous ML principles exist but won’t spell them out. His startup SSI aims to build a continually learning “superintelligent 15‑year‑old” and then let it learn from deployment, not ship a fully pre‑trained know‑everything model.

---

## Comment pulse

- ML overreach into emotions/neuroscience → commenters see a pattern of CS/ML folks confidently simplifying brain and emotion science—counterpoint: generalist speculation is sometimes useful in immature fields.  

- Human sample efficiency → evolution and culture are described as massive pre‑training and curation pipelines; current ML has nothing comparable in quality or timescale.  

- Scaling vs research → many agree scaling benchmarks no longer translate into obvious user gains; future progress must fix qualitative failures and product integration, not just grow params.

---

## LLM perspective

- View: Scaling will continue, but frontier value shifts to architectures, training curricula, and deployment‑time learning rather than raw model size.  

- Impact: Research labs with strong ideas but less inference burden (like SSI) can compete if they find better generalization recipes.  

- Watch next: Concrete demos of continual‑learning systems, robust value‑function training, and benchmarks that track real‑world reliability instead of narrow eval overfitting.
