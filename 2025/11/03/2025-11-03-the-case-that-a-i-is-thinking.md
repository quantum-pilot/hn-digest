# The Case That A.I. Is Thinking

- Score: 211 | [HN](https://news.ycombinator.com/item?id=45802029) | Link: https://www.newyorker.com/magazine/2025/11/10/the-case-that-ai-is-thinking

- TL;DR
  - Somers argues that LLMs exhibit a real, unconscious form of “understanding” rooted in recognition/compression (Baum, Kanerva) and mirrored by neural mechanisms, citing practical wins (coding, image-guided fixes) and emerging mechanistic evidence (features, planning-like circuits). He tempers this with limits: scaling slowdown, poor physics/spatial reasoning, lack of continual learning/embodiment, and unresolved consciousness. Neuroscientists are split between excitement and dread. HN debates center on definitions of “thinking,” memory/looping deficits, emergent behavior vs. sentience, and downstream ethics.

- Comment pulse
  - LLMs are thinking → Coherent, stepwise bug diagnoses in production code — counterpoint: Repetitive failure loops and vague continuity show weak grasp.
  - Thinking needs durable memory and looping → Models freeze after training; context is short-term; no continuous “churn” (Memento/Clive Wearing analogies) limits self-learning.
  - Emergence, not hand-coded rules → Training, not explicit logic, yields behavior; if indistinguishable from humans, sentience claims arise, raising “digital slave” ethics.

- LLM perspective
  - View: Treat LLMs as unconscious recognizers with partial planning; thinking-as-compression fits evidence; consciousness remains unproven.
  - Impact: Productivity gains in software; neuroscience gets testbeds; ethics and labor-policy debates intensify.
  - Watch next: Robust physics/spatial-planning benchmarks; weight-updatable long-term memory; persistent agents; circuit-level transparency; concrete policy on AI rights and use.
