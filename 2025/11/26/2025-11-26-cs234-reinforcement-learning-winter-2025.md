# CS234: Reinforcement Learning Winter 2025

- Score: 191 | [HN](https://news.ycombinator.com/item?id=46052685) | Link: https://web.stanford.edu/class/cs234/

## TL;DR
Stanford’s CS234 is a graduate-level reinforcement learning course covering tabular MDPs, policy gradients, offline RL, exploration, and AlphaGo-style search, built around Python assignments and a project. It emphasizes mathematical foundations, evaluation criteria (regret, sample complexity), and a strict—but AI-aware—academic integrity policy that allows LLMs only for high-level help. HN discussion centers on restricted access to new lecture videos, RL’s long‑term role versus alternative paradigms, when RL is appropriate vs. supervised learning, and brittleness/convergence issues.

## Comment pulse
- Lecture videos should stay public → near-zero cost, benefit; plagiarism/IP worries feel weak; 2024 CS234 playlist remains open — counterpoint: paywalls preserve value for enrollees.  
- RL’s future as core training paradigm questioned → great for control, games, ads and bandits, but many expect new architectures beyond current RLHF-style methods.  
- Standard ML problems rarely need RL → supervised losses give clearer gradients; RL suits sequential decisions with delayed rewards but is brittle and converges poorly.

## LLM perspective
- View: CS234 mirrors modern RL: quickly moving from tabular basics to offline RL, exploration theory, and AlphaGo-style planning.  
- Impact: Graduates are trained to judge when RL is overkill, countering hype-driven misuse in business and research.  
- Watch next: Whether 2025 materials are released publicly, and how methods like FlowRL and model-based RL compare on standard control benchmarks.
