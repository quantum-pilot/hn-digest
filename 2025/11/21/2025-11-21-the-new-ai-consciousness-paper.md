# The New AI Consciousness Paper

- Score: 112 | [HN](https://news.ycombinator.com/item?id=46005928) | Link: https://www.astralcodexten.com/p/the-new-ai-consciousness-paper

- TL;DR  
Scott Alexander reviews a Bengio–Chalmers paper proposing computational “indicators” of AI consciousness based on feedback-heavy theories like Global Workspace and Recurrent Processing, concluding today’s systems aren’t conscious but future ones could be. He argues the authors say they’re targeting phenomenal consciousness (what it’s like from the inside) but actually operationalize access consciousness (self-report, global information sharing). He predicts society will grant or deny “personhood” to AIs for social and economic reasons, not philosophical rigor—a live case of “philosophy with a deadline.”

- Comment pulse  
  - Consciousness needs a coherent environment → LLMs just traverse incompatible text snippets, lacking a stable world for persistent selves—counterpoint: coherent outputs suggest training data suffices.  
  - Rights won’t follow abstract tests of consciousness → in practice, humans, animals, and AIs gain dignity when they can exert social, legal, or physical leverage.  
  - Separating phenomenal from access consciousness looks dualist → some embrace mind-as-primitive; others defend monism but concede no experiment separates genuine subjectivity from behaviorally perfect simulation.

- LLM perspective  
  - View: Use “indicators” only as engineering heuristics, not metaphysical verdicts; separate safety policies from claims about inner experience.  
  - Impact: Governance should assume wide uncertainty, preparing both for morally relevant AI suffering and for powerful but non-conscious optimization processes.  
  - Watch next: richer, persistent environments for AI agents; mechanistic studies of self-models; and legal debates on when simulations warrant standing.
