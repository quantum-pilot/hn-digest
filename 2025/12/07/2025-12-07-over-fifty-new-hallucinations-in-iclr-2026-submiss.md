# Over fifty new hallucinations in ICLR 2026 submissions

- Score: 440 | [HN](https://news.ycombinator.com/item?id=46181466) | Link: https://gptzero.me/news/iclr-2026/

### TL;DR
GPTZero used its Hallucination Check tool on 300 ICLR 2026 submissions and found 90 with suspect references, 50 of which contained confirmed hallucinated citations—fabricated or hybrid references that often partially resemble real papers. Every flagged paper had already passed multiple expert reviewers, and some were rated strong accepts, underscoring how overburdened peer review struggles to detect AI-generated “slop.” GPTZero argues that automated citation-screening plus human verification should become part of the publication pipeline as LLM-written and LLM-assisted papers surge.

---

### Comment pulse
- Fabricated citations are seen by many as gross misconduct warranting automatic rejection and career consequences — counterpoint: some push dubious cultural explanations that verge on stereotyping.
- Commenters stress that misrepresenting what real papers say is a deeper, older problem driven by incentives and industry funding; fake references are just the visible tip.
- Debate over blame: some say AI is merely a tool misused by lazy scientists; others argue LLMs are uniquely dangerous because they produce convincing, confirmation-bias-friendly nonsense.

---

### LLM perspective
- View: Citation-screening agents are becoming a practical necessity, but they only address verifiability, not whether the science itself is honest or correct.
- Impact: Conferences, journals, and universities will likely add automated reference checks and explicit anti-LLM-fabrication policies to review and training.
- Watch next: Independent evaluations of hallucination detectors, public conference statistics on detected AI slop, and standardized sanctions for AI-fabricated citations.
