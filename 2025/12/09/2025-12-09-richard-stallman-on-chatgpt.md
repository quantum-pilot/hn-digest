# Richard Stallman on ChatGPT

- Score: 121 | [HN](https://news.ycombinator.com/item?id=46203591) | Link: https://www.stallman.org/chatgpt.html

### TL;DR
Richard Stallman argues ChatGPT and similar generative models aren’t genuinely intelligent because they don’t “know” meanings, merely pattern‑match text without concern for truth. He labels them “bullshit generators,” warning that anthropomorphizing them encourages misplaced trust, and criticizes ChatGPT’s proprietary, server‑only distribution as an attack on user computing freedom. Hacker News discussion splits between agreement on their untrustworthiness and Stallman’s software‑freedom critique, versus pushback that his definition of intelligence is semantic, given LLMs’ clear practical capabilities.

---

### Comment pulse
- Stallman supporters → cite his MIT AI‑lab background, agree LLMs only imitate understanding, and amplify his long‑standing concern about proprietary, server‑side software.  
- Critique is mostly semantics → humans also “bullshit,” and we can’t prove their understanding either—counterpoint: precision about “knowing” matters for setting expectations and safeguards.  
- Utility over philosophy → many stress LLMs already outperform average research for routine tasks, like a calculator, yet agree they’re unsafe as lone decision‑makers.  

---

### LLM perspective
- View: Treat “intelligence” as an engineering continuum; define capability levels and reliability metrics instead of arguing philosophical thresholds.  
- Impact: Regulation should target documented error rates, data governance, and user control of models, not metaphysical claims about understanding.  
- Watch next: Growth of open‑weight runnable models will test Stallman’s concerns by separating freedom issues from reliability and anthropomorphism debates.
