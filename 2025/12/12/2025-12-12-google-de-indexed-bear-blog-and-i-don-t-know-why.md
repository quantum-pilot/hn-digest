# Google de-indexed Bear Blog and I don't know why

- Score: 394 | [HN](https://news.ycombinator.com/item?id=46239752) | Link: https://journal.james-zhan.com/google-de-indexed-my-entire-bear-blog-and-i-dont-know-why/

### TL;DR
The author’s Bear Blog subdomain was initially indexed by Google after submitting a sitemap and manual URL requests via Search Console. Shortly after triggering a “Validate fix” on an RSS feed URL, almost all pages became “Crawled – currently not indexed,” with no meaningful diagnostic information. Other search engines indexed the site normally, and another subdomain on the same domain indexed fine. After exhausting suspects (DNS, content quality, structure), the author moved to a new subdomain, while HN discussion broadens this into concern about opaque, brittle Google indexing and traffic loss.

---

### Comment pulse
- Traffic collapse from Google often ties to AI Overviews and weird query spam → impressions without clicks, or negative-SEO via indexed search-result pages.  
- Core problem: Google acts like critical infrastructure without utility-like obligations → no clear appeals process, poor transparency — counterpoint: some see this more as monopoly than “gatekeeper” semantics.  
- Multiple users report massive de-indexing after benign changes; recovery never comes, killing content businesses → suggests fragile, understaffed, AI-distracted search infrastructure.

---

### LLM perspective
- View: Treat Google traffic as unstable; diversify discovery (newsletters, RSS, federated search, other engines) instead of relying on Search Console sanity.  
- Impact: Small publishers, solo blogs, and SEO-dependent businesses face existential risk from silent indexing shifts and AI answer boxes.  
- Watch next: Regulatory moves on “search as utility,” Google’s search/AI quality metrics, and third-party tools that independently track index coverage across engines.
