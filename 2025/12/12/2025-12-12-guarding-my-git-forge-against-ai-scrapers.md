# Guarding My Git Forge Against AI Scrapers

- Score: 151 | [HN](https://news.ycombinator.com/item?id=46241849) | Link: https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/

- TL;DR  
  The author’s small self‑hosted Forgejo instance was overwhelmed by AI and commercial scrapers, generating hundreds of thousands of requests per day, spiking CPU, bandwidth, and electricity bills. Log analysis showed tens of thousands of IPs from clouds, VPNs, and even residential ISPs, often in coordinated waves. Nginx caching and rate limiting either failed or degraded legitimate use. The effective solution was Iocaine 3 as middleware, classifying bots and feeding them autogenerated “garbage maze” pages, which restored usability and hardened the forge against mass data extraction.

- Comment pulse  
  Use built‑in or front‑door auth on forges → Gitea's REQUIRE_SIGNIN_VIEW=expensive, VPNs, or oauth2-proxy hide expensive pages and slashed some users' CPU/bandwidth.  
  Use Anubis-style proof-of-work challenges → one admin saw hits drop from 600k to 1k/day, so captchas deter scrapers — counterpoint: PoW burdens weak clients.  
  Geo-block obvious abuse regions → blocking countries where you don't serve users can cut bot traffic substantially — counterpoint: undermines borderless-internet ideal and excludes foreigners.

- LLM perspective  
  View: AI and commercial scraping now behave like slow DDoS; defenses must merge web security, abuse mitigation, and data-governance.  
  Impact: Small self-hosters and niche communities bear disproportionate resource costs while providing free training data to large, profit-seeking model operators.  
  Watch next: Expect more middleware like Iocaine, forge-level "expensive view" features, and regulatory moves on unauthorized training data and bot labeling.
