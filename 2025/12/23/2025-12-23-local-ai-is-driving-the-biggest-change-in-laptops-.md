# Local AI is driving the biggest change in laptops in decades

- Score: 103 | [HN](https://news.ycombinator.com/item?id=46360856) | Link: https://spectrum.ieee.org/ai-models-locally

- TL;DR
    - Local AI still strains most laptops, so vendors are redesigning PCs around NPUs, unified memory, and OS-level runtimes to run models offline. Qualcomm, AMD, Intel and Microsoft’s Copilot+ / AI Foundry efforts chase more TOPS within tight power and memory budgets for assistants and media tools. Commenters argue some existing machines already manage local LLMs, mock the “unknown TOPS” line, and see “AI PCs” as largely unwanted marketing hype.

- Comment pulse
    - Apple laptops already run local LLMs well via unified memory and GPU; NPUs mainly improve efficiency—counterpoint: costly RAM and weaker models keep cloud attractive.
    - The “no one knows TOPS needed” claim is ridiculed → FLOPs-per-token is straightforward to approximate; VRAM decides model size, TOPS mostly affects latency.
    - Many distrust “AI PCs” → marketing oversells Copilot and NPUs; current AI features are cloud-based or useless to users—counterpoint: extra NPU silicon rarely harms anything.

- LLM perspective
    - View: Local LLMs will complement, not replace, cloud models; frontier-scale reasoning remains economically and technically centralized for years.
    - Impact: Laptops adopt phone-like SoCs and unified memory, trading upgradeability for cooler, quieter systems that tolerate sustained AI background tasks.
    - Watch next: Independent benchmarks comparing local vs cloud cost, latency, privacy for realistic workflows will reveal when AI PCs truly matter.
