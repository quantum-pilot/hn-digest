# Tell HN: HN was down

- Score: 462 | [HN](https://news.ycombinator.com/item?id=46301921) | Link: https://news.ycombinator.com/item?id=46301921

## TL;DR
Hacker News had a multi-hour outage likely caused by crawlers overloading the site after anti-crawler settings were relaxed, compounded by a human monitoring miss when PagerDuty woke the maintainer at night and the issue was mistakenly cleared. The web UI was down while some CLI/API-style access still worked. Commenters mostly responded with humor about their HN addiction and muscle memory, plus some thoughtful discussion about sustainable on-call practices, sleep, and better crawler/rate-limiting strategies.

*Content unavailable; summarizing from title/comments.*

## Comment pulse
- Outage cause framed as crawler overload after relaxing anti-bot defenses → maintainer tries not to harm users, but mis-tuned knobs exposed HN to scrapers.  
- On-call fatigue discussed → PagerDuty woke maintainer at 5:24am; some suggest silencing night alerts to protect sleep—counterpoint: reliability relies on timely human checks.  
- Users confront HN dependency → reflexively opening HN, infinite reload loops, altered morning routines; some adopt tools like LeechBlock or no-procrast modes to curb visits.  

## LLM perspective
- View: Single-human, pager-based ops for a high-traffic community site is fragile; crawler policy needs automation, not manual knob-twiddling.  
- Impact: Heavy HN users rediscover their reliance on one site for news, discussion, and procrastination when it briefly disappears.  
- Watch next: Better rate limits, crawler channels, clearer status/metrics, and possibly a rotating on-call team instead of a single moderator.
