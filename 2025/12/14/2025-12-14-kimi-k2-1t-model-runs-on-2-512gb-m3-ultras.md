# Kimi K2 1T model runs on 2 512GB M3 Ultras

- Score: 198 | [HN](https://news.ycombinator.com/item?id=46262734) | Link: https://twitter.com/awnihannun/status/1943723599971443134

### TL;DR
A 1T-parameter Kimi K2 model has been demonstrated running across two 512 GB M3 Ultra Mac Studios, implying “prosumer” local inference is technically possible but very expensive and likely latency-bound at long context lengths. Commenters say Kimi K2 stands out less for raw intelligence and more for sharp editing, emotional nuance, and willingness to bluntly challenge the user—making it unusually good for communication-heavy tasks. Discussion turns to hardware costs, Linux/AMD alternatives, and skepticism about real-world speed with large contexts.

*Content unavailable; summarizing from title/comments.*

---

### Comment pulse
- Kimi excels at short-form writing and “emotional intelligence” → strong at tone, social norms, calling out user errors; top EQ-bench scorer — counterpoint: pushback may conflict with strict instruction-following.

- Users value its un-obsequious style → it will roast, critique, and challenge reasoning, unlike more flattering models; seen as a great editor and sanity-checker.

- Hardware side: 512 GB M3 Ultras cost ~$9.5k each, less via refurbs/gift cards; commenters ask about Linux/RDNA/10G-NIC equivalents and warn latency may explode with long contexts.

---

### LLM perspective
- View: This setup showcases the upper edge of “desktop” LLM hosting, but cost, power, and latency keep it niche.

- Impact: Communication-focused professionals may prioritize Kimi-like social acuity over slight reasoning gains from frontier models.

- Watch next: Independent throughput/latency tests at max context and Linux multi-GPU/RDNA configurations matching or beating dual M3 Ultra.
