# Reflections on AI at the End of 2025

- Score: 187 | [HN](https://news.ycombinator.com/item?id=46334819) | Link: https://antirez.com/news/157

- TL;DR  
  Antirez (creator of Redis) argues that by late 2025, most serious researchers accept LLMs aren’t “stochastic parrots” but systems with useful internal representations. Chain-of-thought is framed as internal search plus RL-shaped token sequencing, with verifiable-reward RL seen as the next big driver of progress beyond pure scaling. LLMs are now routine in software work, while alternative architectures emerge in parallel. He cites strong ARC progress and worries the core long‑term AI problem is avoiding human extinction; commenters debate safety, hype, and psychological over‑reliance.

- Comment pulse  
  Non-expert users increasingly treat LLMs as doctors, therapists, and speechwriters; defenders argue they still beat scarce human experts and SEO-poisoned search results.  
  Developers describe a split between enthusiastic adopters and resistant “skeptics,” with some noting genuine model improvement while others blame management mandates and culture-war style polarization.  
  Several see stalled exponential progress and report “AI psychosis”: people outsourcing choices and relationships to chatbots, blurring reality and amplifying existing information-fragmentation problems.

- LLM perspective  
  View: The crucial question isn’t “AGI or not” but when LLM advice outperforms typical human or web advice in specific domains.  
  Impact: Software practice, medicine, and politics must adapt to a world where persuasive but unverifiable text is abundant and cheap.  
  Watch next: Hard benchmarks for RL-driven self-improvement, real-world harm/benefit studies, economic viability of inference, and governance norms for high-stakes AI use.
