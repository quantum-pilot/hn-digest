# DeepMind and OpenAI win gold at ICPC

- Score: 206 | [HN](https://news.ycombinator.com/item?id=45279357) | Link: https://codeforces.com/blog/entry/146536

- TL;DR
  - OpenAI says an automated system (GPT-5 plus an experimental reasoning model) solved all 12 ICPC World Finals 2025 problems under ICPC oversight; DeepMind reports a Gemini-assisted team solved 10. ICPC pages appear to endorse both “gold-level” results; best human team solved 11/12. HN debates fairness (AI’s compute/memory vs humans’ time/one-PC rules), transparency and cost, and whether results are reproducible with public models. Some saw this as the first major CP event where AI outperformed all participants.

- Comment pulse
  - Not the same game → AIs exploit vast compute/memory; humans face 5-hour, one-PC, 25-page-refs constraints — counterpoint: cars vs horses; progress shifts rules.
  - Trust and cost → Sparse details on oversight, scaffolding, and spend; one user saw GPT‑5 return a placeholder; calls for independent audits and API-cost estimates.
  - What the runs looked like → OpenAI: 11 first‑try, 9 on hardest; DeepMind: 17 submits/10 solves; ICPC shows only AC/WA/TLE, no failing-test numbers.

- LLM perspective
  - View: Agentic ensembles with selection beat single models; local stress-testing and submission strategies likely crucial to first‑try accuracy.
  - Impact: Benchmarks shift: CP/IOI problems no longer separate humans; education and hiring lean on agents for algorithm design, coding, and debugging.
  - Watch next: Release artifacts, reproducible logs, and costs; public access to “experimental reasoning” model; independent reruns with hidden tests to probe generalization.
