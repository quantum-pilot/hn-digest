# Qwen3-Next

- Score: 526 | [HN](https://news.ycombinator.com/item?id=45219228) | Link: https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list

- TL;DR
  - Alibaba’s Qwen3-Next targets extreme efficiency: a 75% Gated DeltaNet (linear) + 25% enhanced standard attention mix, ultra‑sparse 512‑expert MoE activating ~3B of 80B (≈3.7%), stability-oriented norms/gating, and native multi‑token prediction (MTP) enabling high‑acceptance speculative decoding. Trained on 15T tokens at ~9.3% of Qwen3‑32B’s compute, it beats Qwen3‑32B, rivals the 235B flagship (and wins at 256K), and delivers >10× throughput beyond 32K. HN highlights memory‑saving MTP, practical 262K/32K limits with optional YaRN to 1M, and mixed early coding scores (~GPT‑OSS‑20B).

- Comment pulse
  - MTP sans extra unembedding saves several GB and raises speculative-decoding acceptance/throughput; unlike Medusa, it reuses weights, keeping active parameters small.
  - Ultra‑long context feels fast; native 262K input, 32K generation; YaRN can extend to 1M — counterpoint: quality often drops when sequences approach the limit.
  - On Brokk Open Round (coding), performance ≈ GPT‑OSS‑20B; usefulness debated due to Java‑only tasks and mismatch with developers’ real workloads.

- LLM perspective
  - View: Hybrid linear+standard attention plus ultra‑sparse MoE and native MTP push long‑context throughput without ballooning VRAM.
  - Impact: Serving stacks must expose MTP-aware speculative decoding; cheaper 256K deployments become practical on 4–8 GPUs.
  - Watch next: HF/vLLM first-class MTP support, independent 256K–1M accuracy audits, and cost-normalized head‑to‑head vs DeepSeek, Kimi, Llama.
