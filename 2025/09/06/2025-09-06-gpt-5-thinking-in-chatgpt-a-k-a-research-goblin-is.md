# GPT-5 Thinking in ChatGPT (a.k.a. Research Goblin) is good at search

- Score: 358 | [HN](https://news.ycombinator.com/item?id=45152284) | Link: https://simonwillison.net/2025/Sep/6/research-goblin/

- TL;DR
  - Simon Willison argues GPT-5 “Thinking” in ChatGPT is now a competent research assistant: it interleaves web search with reasoning, cites sources, handles images/PDFs, and proposes next steps—from identifying buildings to teasing apart UK cake-pop availability or tracing Exeter quay vaults. He prefers it on mobile and over Deep Research. HN readers agree on usefulness for messy, open-ended queries, but warn about speed, costs, consensus bias, and hallucinations; they ask for confidence signals and better source evaluation, and debate search-vs-memory trade-offs.

- Comment pulse
  - Assistants shine on fuzzy, multi-step tasks → rapidly triage many sources and synthesize candidates — counterpoint: can cement consensus without probing bias or dissenting evidence.
  - For simple lookups, manual Google + skim is faster and cheaper → GPT-5 Thinking adds latency, verification overhead; users want confidence scores and token/energy costs.
  - Web-grounded models with citations beat “internal knowledge” for trust → others miss encyclopedic recall; a mode switch between search and memory helps.

- LLM perspective
  - View: Interleaved tool use plus transparent thinking is the unlock; default should scale depth to query difficulty, not always “go deep.”
  - Impact: Shifts everyday research to agents; pressures SEO-driven content, documentation quality, and mobile voice UX norms.
  - Watch next: Calibrated confidence per claim, per-source reliability, and cost meters; safe agent actions like emailing, with strict approvals and audit trails.
