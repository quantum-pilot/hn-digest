# OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems

- Score: 374 | [HN](https://news.ycombinator.com/item?id=45335474) | Link: https://openai.com/index/openai-nvidia-systems-partnership/

- TL;DR
    - OpenAI and Nvidia signed a letter of intent to deploy at least 10 GW of Nvidia AI systems, with Nvidia investing up to $100B as capacity comes online; first 1 GW targets H2 2026 on the Vera Rubin platform. The aim is co-optimized hardware/software for next‑gen model training and inference. HN focuses on the power framing (grid capacity, rates, cost allocation), vague “intent” language, and sheer scale (millions of GPUs). Debate spans bubble/“peak LLM” vs early‑innings optimism, plus site‑specific water worries.

- Comment pulse
    - GW framing triggers grid/bill fears → 10 GW ~ big cities; rates rose; who pays? — counterpoint: watts are standard; regulators can allocate costs.
    - Press-release hedging irks readers → LOI, 'intends to invest up to,' 'preferred partner' sounds non-committal; looks like stock-boosting signal.
    - Scale estimates → 3–10M GPUs depending on PUE/overhead; rough rules: ~3 MW per 1k GPUs, NVL72 rack ~120 kW plus cooling; cabling is massive.

- LLM perspective
    - View: LOI signals vertical co-planning: Nvidia buys into OpenAI to guarantee multi-year demand; OpenAI trades flexibility for capacity and roadmap influence.
    - Impact: Real execution would centralize AI compute further; power markets, permitting, and supply chains face pressure; smaller labs risk being capacity-constrained.
    - Watch next: Binding contracts, sites, PPAs, interconnect queues, Vera Rubin performance, delivery cadence, governance conditions tied to the up-to-$100B Nvidia investment.
