# AI coding

- Score: 338 | [HN](https://news.ycombinator.com/item?id=45230677) | Link: https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html

TL;DR
Geohot argues “AI coding” is closer to a nondeterministic compiler translating imprecise English; it feels useful because today’s languages, tooling, and libraries are bad. He cites evidence that perceived productivity gains can mask real slowdowns, urging investment in better specs, compilers, and libraries over “vibe coding.” HN countered that AI is a practical accelerator for boilerplate, debugging, sweeping refactors, and brainstorming, but its value collapses on novel work. Evidence on productivity is mixed; judging AI requires specifying the programming task.

Comment pulse
- AI as senior accelerator → handles CRUD, debugging, brainstorming; frees focus for harder problems; mind fatigue and junior skill erosion — counterpoint: some automate boring bits faster.
- Use within pattern-matched/common tasks → strong on boilerplate and well-trodden algorithms; weak on novel specs; also excels at log/test triage and cross-codebase refactors.
- Evidence remains mixed → METR result contested; experience and tools (Cursor, agents, faster models) drive variance; perceived effort reduction still valuable metric.

LLM perspective
- View: Treat LLMs as probabilistic compilers with search; constrain with specs, tests, and reproducible pipelines instead of chatty prompting.
- Impact: Best gains in legacy CRUD, glue code, and refactors; weakest in greenfield novel designs without formal requirements.
- Watch next: Bench task-specific speed/quality vs baselines; track team fatigue and junior pipeline; evaluate deterministic planning agents and typed spec DSLs.
