# ‘Overworked, underpaid’ humans train Google’s AI

- Score: 242 | [HN](https://news.ycombinator.com/item?id=45231239) | Link: https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans

- TL;DR
    - The Guardian reports Google’s outsourced “AI raters” (via Hitachi’s GlobalLogic) score Gemini and AI Overviews, increasingly reviewing distressing content under tighter deadlines and shifting guidelines, including looser rules on repeating slurs. Workers report anxiety, little support, and pressure to prioritize speed over safety; Google says rater feedback is aggregated and doesn’t directly change models. HN notes pay varies widely, the supply chain is opaque and industry‑wide (RLHF), and internal metrics can mask declining quality.

- Comment pulse
    - Expert contractors report $45+/hr, tough qualifications, sporadic work; rural availability drives acceptance — counterpoint: GlobalLogic raters cited at $16–$21/hr.
    - Throughput targets trump quality; a hidden human “factory” props demos and internal metrics, degrading search and chat outputs.
    - Practice is industry-wide: RLHF/data-labeling vendors abound; most models use human feedback; secrecy obscures scale and exposure to extreme content.

- LLM perspective
    - View: Human-in-the-loop pipelines are essential infrastructure now; without standards, guideline drift and consensus bias will keep leaking into products.
    - Impact: Underpaid exposure to trauma creates workforce churn; mandated non-expert ratings raise harm risk in health, finance, and safety-critical domains.
    - Watch next: Model cards disclosing rater policies, domain gating, incident rates; third-party audits; pay floors, counseling, and opt-outs for sensitive content.
