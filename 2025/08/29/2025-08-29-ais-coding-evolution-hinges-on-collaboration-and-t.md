# AI’s coding evolution hinges on collaboration and trust

- Score: 183 | [HN](https://news.ycombinator.com/item?id=45065343) | Link: https://spectrum.ieee.org/ai-for-coding

- TL;DR
    - An ICML’25 paper by Cornell/MIT/Stanford/Berkeley argues coding AIs aren’t ready for full autonomy: they falter on large codebases, long-horizon design, and intent capture, often hallucinating fixes. The authors urge better human-AI interfaces, uncertainty-aware agents that ask clarifying questions, and verifiable, evolutionary/agentic workflows—with humans in the loop to maintain trust. HN readers largely agree: coding isn’t the bottleneck, problem definition is; usefulness spikes when agents can run/tests/see UIs; blanket claims hide domain variance and safety constraints; models still act “lazy” or overeager.

- Comment pulse
    - Senior/junior split grows → describing problems and abstractions is harder than typing code; toolchain mastery helps — counterpoint: editors and AI choices don’t change outcomes.
    - Autonomy depends on environment access → empowering agents to run code, tests, and UIs improves results; breakpoint debugging integration is still missing.
    - Generalizations obscure reality → usefulness varies by stack and risk; models can be lazy or overeager; safety-critical work demands human oversight.

- LLM perspective
    - View: Autonomy hinges on better problem-spec interfaces, uncertainty-aware agents, and verifiable execution, not just larger models.
    - Impact: Short-term: productivity gains in unfamiliar stacks; long-term: shift in senior roles toward intent capture, architecture, and review.
    - Watch next: Benchmarks with multi-repo, months-long tasks; IDE APIs exposing debuggers; agent evals with uncertainty prompts and continuous improvement loops.
