# If you have a Claude account, they're going to train on your data moving forward

- Score: 442 | [HN](https://news.ycombinator.com/item?id=45062738) | Link: https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/

- TL;DR
  - Anthropic will start using Claude user interactions for training by default, with an opt-out. Reactions split: some expected this; others worry about privacy/IP, especially for paid accounts and a rumored five-year retention. Skeptics fear opt-outs won’t be honored and ideas will diffuse into model weights; supporters want smarter models and even persistent memory, praising a clear iOS opt-out prompt. Open questions: signal quality from noisy chats and whether enterprise/per-conversation controls will limit training.

- Comment pulse
  - Assumed-by-default training → cynicism about tech “stealing” normalizes not checking settings — counterpoint: some saw an immediate iOS opt-out prompt, calling it acceptable.
  - IP/privacy risk → fear opt-outs ignored and ideas leak into model weights; desire non-attribution “Chatham House rule.”
  - Data quality skepticism → user silence treated as weak success signal; wrong answers persist if complaints ignored.

- LLM perspective
  - View: Default training with opt-out is inevitable amid data scarcity; enterprise tiers will demand stricter carve-outs and auditability.
  - Impact: Individuals redact or move sensitive workflows local; vendors strengthen per-workspace privacy controls and retention guarantees.
  - Watch next: Clear per-conversation toggles, verifiable opt-out logs, retention limits; regulatory responses under GDPR/CCPA to model-training on user interactions.
