# Updates to Consumer Terms and Privacy Policy

- Score: 756 | [HN](https://news.ycombinator.com/item?id=45062683) | Link: https://www.anthropic.com/news/updates-to-our-consumer-terms

TL;DR
Anthropic updated consumer terms: users can allow chats/code to train future Claude models and safety systems. Decision required by September 28, 2025; applies only to new/resumed sessions. Opt-in enables five-year retention; opt-out keeps 30 days. Deleted chats excluded from future training; commercial, API, government/education excluded. Turning off later stops future use, but data in ongoing or completed training remains. Rationale: real-world data improves quality and misuse detection. HN debates consent UX and defaults, IP leakage risks, and industry data hunger, with some welcoming opt-out clarity.

Comment pulse
- Consent feels dark-patterned → Ambiguous slider colors, opt-in framed as T&C update, five-year retention seen excessive — counterpoint: pop-up clearly offered opt-out.
- Risk of leaking novel work → Researchers fear AI generalizes unpublished ideas to others; advice: keep secrets separate or opt out.
- Data scarcity drives policy → Web corpora tapped; lawsuits brewing; firms need fresh human data; calls for guardrails to curb adtech-LLM abuses.

LLM perspective
- View: Consent-based training aligns with industry; five-year retention is aggressive and risks eroding user trust.
- Impact: Consumers weigh capability gains against privacy; enterprise/API exempt; expect modest churn and improved misuse classifiers.
- Watch next: Opt-in rates, filtering efficacy metrics, differential privacy or federated approaches, and EU/California regulator responses.
